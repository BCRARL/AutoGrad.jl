2016-08-18  Deniz Yuret  <dyuret@ku.edu.tr>

	* profiling:

	(1) AutoGrad: Testing 1 epoch h=64 mnist with minibatch=100, best out of 3 on ural from fresh start.
	0. Regular train (record, update)
	1. Forward back (record, no update)
	2. Nonrecording, no update, just forward (computes softloss)
	3. Just predict, no softloss.
	4. forward_pass(loss) (recording version of #2)

	0. 13.944595 seconds (3.95 M allocations: 2.299 GB, 18.09% gc time)
	1. 13.100620 seconds (3.93 M allocations: 1.844 GB, 15.03% gc time)
	2. 4.288226 seconds (64.38 k allocations: 114.196 MB, 0.17% gc time)
	3. 4.242340 seconds (33.18 k allocations: 98.294 MB, 0.12% gc time)

	(2) Problem: mixing Float64 weigths with Float32 data!

	(3) AutoGrad: Use Float64 weights and data
	0. 6.512184 seconds (3.94 M allocations: 2.299 GB, 38.67% gc time)
	1. 5.719212 seconds (3.92 M allocations: 1.843 GB, 34.23% gc time)
	2. 0.483972 seconds (60.78 k allocations: 114.032 MB, 1.31% gc time)
	3. 0.444158 seconds (29.58 k allocations: 98.129 MB, 1.01% gc time)

	(4) AutoGrad: Use Float32 weights and data
	0. 4.275396 seconds (3.94 M allocations: 1.237 GB, 33.43% gc time)
	1. 3.702224 seconds (3.92 M allocations: 1.008 GB, 32.20% gc time)
	2. 0.269955 seconds (60.78 k allocations: 58.239 MB, 1.22% gc time)
	3. 0.242950 seconds (29.58 k allocations: 49.606 MB, 1.13% gc time)

	(5) Continue with Float32.

	(6) AutoGrad: gc_enable(false) while measuring time:
	0. 2.760838 seconds (3.94 M allocations: 1.236 GB)
	1. 2.489116 seconds (3.92 M allocations: 1.007 GB)
	2. 0.266896 seconds (60.78 k allocations: 58.239 MB)
	3. 0.240501 seconds (29.58 k allocations: 49.606 MB)

	(7) AutoGrad: use axpy! during update:
	0. 2.559526 seconds (3.92 M allocations: 1.008 GB)

	(8) Compare with Knet: same task.
	0. 1.128186 seconds (8.94 M allocations: 141.341 MB, 1.63% gc time) Forw-back-update
	1. 1.244718 seconds (10.76 M allocations: 168.832 MB, 1.70% gc time) Forward-back (no update)
	2. 0.590746 seconds (5.39 M allocations: 84.441 MB, 1.41% gc time) Forward only

	(9) broadcast vs vectorized ops. (testing exp(a) where a=rand(10000,10000))
	2.989990 seconds (2 allocations: 762.940 MB): exp(a)
	2.920066 seconds (12 allocations: 762.940 MB, 0.22% gc time): broadcast(exp,a)
	2.725942 seconds (1 allocation: 32 bytes): broadcast!(exp,b,a)
	2.724059 seconds (1 allocation: 32 bytes): broadcast!(exp,a,a)
	Not much difference, vectorized ops to be deprecated in favor of broadcast in v6.0.

	(10) Julia v0.5 is not faster (but maybe it hasn't been compiled optimally).
	0. 2.858207 seconds (3.78 M allocations: 1.011 GB)

	(11) Profile1
    2276 ...AutoGrad/src/core.jl; gradfun; line: 36  backward_pass(forward_pass(fun, args, kwargs, argnum)...)
     242 ...AutoGrad/src/core.jl; backward_pass; line: 208  cur_outgrad = sum_outgrads(node.outgrads...)
      237 ...utoGrad/src/core.jl; sum_outgrads; line: 570
       192 ...utoGrad/src/core.jl; sum_outgrads; line: 570
        187 broadcast.jl; broadcast; line: 253
         142 broadcast.jl; broadcast!; line: 246
     711 ...AutoGrad/src/core.jl; backward_pass; line: 214  og = gradfun(cur_outgrad)
      415 .../src/collections.jl; anonymous; line: 14       getindex(::D1,y,x,i...) = dy->ungetindex(x,dy,i...)
       412 no file; ungetindex; line: 0
        298 ...utoGrad/src/core.jl; r; line: 126
         298 .../src/collections.jl; ungetindex; line: 21
          297 ...toGrad/src/core.jl; fill_internal; line: 546
           295 ...oGrad/src/core.jl; fill_check; line: 549
            291 ...oGrad/src/core.jl; fill_internal; line: 546
             264 array.jl; fill!; line: 193
      150 ...utoGrad/src/util.jl; anonymous; line: 22
     953 ...AutoGrad/src/core.jl; forward_pass; line: 72   end_node = fun(args...; kwargs...)
      519 .../examples/footime.jl; loss; line: 39          ypred = predict(w, x)
       329 ...examples/footime.jl; predict; line: 32       x = max(0, w[i]*x .+ w[i+1])
        108 no file; .+; line: 0
       186 ...examples/footime.jl; predict; line: 35       return w[i]*x .+ w[i+1]
      260 .../examples/footime.jl; loss; line: 40          ynorm = ypred .- log(sum(exp(ypred),1))
      173 .../examples/footime.jl; loss; line: 41          -sum(ygold .* ynorm) / size(ygold,2)

	(12) Focusing on forward:
	2. 0.266896 seconds (60.78 k allocations: 58.239 MB) call loss
	4. 0.939486 seconds (1.96 M allocations: 146.032 MB) call forward_pass(loss) (recording)

	(13) Taking out all the debug calls (turning them into macros)
	0. 1.556437 seconds (1.12 M allocations: 906.322 MB)
	2. 0.267631 seconds (60.78 k allocations: 58.239 MB)
	4. 0.544930 seconds (587.58 k allocations: 82.464 MB)
	At this point recording takes as much time as forward calculation, backward pass takes 4x more.
	Forward is as efficient as can be, dominated by array ops, except for excessive allocation.

	(14) What costs on forward_pass? (profiling 20 epochs)
    10176 ...AutoGrad/src/core.jl; forward_pass; line: 72  end_node = fun(args...; kwargs...)
     7875 .../examples/footime.jl; loss; line: 39  ypred = predict(w, x)
      6595 ...examples/footime.jl; predict; line: 32  x = max(0, w[i]*x .+ w[i+1])
       810  no file; *; line: 0
       3307 no file; .+; line: 0
       1636 no file; getindex; line: 0
       832  no file; max; line: 0
      1231 ...examples/footime.jl; predict; line: 35  return w[i]*x .+ w[i+1]
       294 no file; *; line: 0
       448 no file; .+; line: 0
       484 no file; getindex; line: 0
     1391 .../examples/footime.jl; loss; line: 40  ynorm = ypred .- log(sum(exp(ypred),1))
      463 no file; .-; line: 0
      440 no file; exp; line: 0
      227 no file; log; line: 0
      255 no file; sum; line: 0
     907  .../examples/footime.jl; loss; line: 41  -sum(ygold .* ynorm) / size(ygold,2)
      160 no file; -; line: 0
      331 no file; .*; line: 0
      198 no file; /; line: 0
      215 no file; sum; line: 0

	Compare with regular call:
   2526 ...d/examples/footime.jl; train2; line: 96  z = loss(w, x, y)
    1604 ...examples/footime.jl; predict; line: 32  x = max(0, w[i]*x .+ w[i+1])
     262 broadcast.jl; .+; line: 315
     392 linalg/matmul.jl; *; line: 132
     943 operators.jl; max; line: 391
    201  ...examples/footime.jl; predict; line: 35  return w[i]*x .+ w[i+1]
     152 broadcast.jl; .+; line: 315
     46  linalg/matmul.jl; *; line: 132
    154  broadcast.jl; .-; line: 321
    324  operators.jl; exp; line: 381
    36   operators.jl; log; line: 381
    52   reducedim.jl; sum; line: 264
    132  sparse/sparsematrix.jl; .*; line: 1026

	Most expensive lines in recorder:
	tapes=Set() # convert to a regular array
	for (arg,i) in enumerate(args) # convert to a regular for

	Next iteration:
	109 ...utoGrad/src/core.jl; r; line: 114  in(tape,tapes) || push!(tapes,tape)
	143 ...utoGrad/src/core.jl; r; line: 127  result = f(argvals...; kwargs...)
	101 ...utoGrad/src/core.jl; r; line: 147  for (tape, argnum, parent) in ops
         83 tuple.jl; indexed_next; line: 21
	76  ...utoGrad/src/core.jl; r; line: 149  gradfun = f(Grad{argnum}, result, args...; kwargs...)

	Removing in(tape,tapes), handle duplicates in Node().
	Next iteration: for loops with tuples; maybe we can optimize the single tape case.

       448 no file; .+; line: 0
        91  ...utoGrad/src/core.jl; r; line: 111  for (tape, parent_rnode) in arg.tapes
         74 operators.jl; indexed_next; line: 437
        109 ...utoGrad/src/core.jl; r; line: 127  result = f(argvals...; kwargs...)
        63  ...utoGrad/src/core.jl; r; line: 147  for (tape, argnum, parent) in ops
         51 tuple.jl; indexed_next; line: 21

	(15) latest single epoch times: (compare to #13)
	0. 1.380165 seconds (905.62 k allocations: 896.819 MB)
	1. 1.344440 seconds (903.22 k allocations: 896.783 MB)
	2. 0.267039 seconds (60.78 k allocations: 58.240 MB)
	3. 0.240020 seconds (29.58 k allocations: 49.606 MB)
	4. 0.434085 seconds (454.38 k allocations: 76.330 MB)

	This is better than Knet with recording in forward_pass.  Time to
	optimize the backward_pass.

	(16) Profiling backward_pass with 10 epochs.

   10496 .../examples/footime.jl; train0; line: 73  g = gradfun(w, x, y)
    10484 ...AutoGrad/src/core.jl; gradfun; line: 36  backward_pass(forward_pass(fun, args, kwargs, argnum)...)	
     2018 ...utoGrad/src/core.jl; backward_pass; line: 209  cur_outgrad = sum_outgrads(node.outgrads...)
     290  ...utoGrad/src/core.jl; backward_pass; line: 213  for (gradfun, parent) in node.parent_grad_ops
     4558 ...utoGrad/src/core.jl; backward_pass; line: 215  og = gradfun(cur_outgrad)
     3472 ...utoGrad/src/core.jl; forward_pass; line: 72  end_node = fun(args...; kwargs...)
   136   .../examples/footime.jl; train0; line: 76  Base.axpy!(-lr, g[i], w[i])

	# sum_outgrads spends significant time adding gradients for fanout>1 ops.
	# we can try adding to an accumulator instead of pushing.
	# we can also use the first gradient as the accumulator to avoid allocation but we decided that was dangerous.
	#   the same gradient matrix is passed back multiple times by e.g. +?
	#   actually right now we are multiplying with 1! never passing the original dy?

     2018 ...utoGrad/src/core.jl; backward_pass; line: 209  cur_outgrad = sum_outgrads(node.outgrads...)
      1996 ...utoGrad/src/core.jl; sum_outgrads; line: 572  sum_outgrads{T}(a::AbstractArray{T},b::AbstractArray{T},c::AbstractArray{T}...) =
       1686 ...utoGrad/src/core.jl; sum_outgrads; line: 572  sum_outgrads{T}(a::AbstractArray{T},b::AbstractArray{T},c::AbstractArray{T}...) =	
        1670 broadcast.jl; broadcast; line: 253
         1493 broadcast.jl; broadcast!; line: 246

     4558 ...utoGrad/src/core.jl; backward_pass; line: 215  og = gradfun(cur_outgrad)

      2514 .../src/collections.jl; anonymous; line: 14  getindex(::D1,y,x,i...) = dy->ungetindex(x,dy,i...) # y=x[i], dy=df/dy
       2491 no file; ungetindex; line: 0
        2287 ...toGrad/src/core.jl; r; line: 127  result = f(argvals...; kwargs...)
         2279 ...src/collections.jl; ungetindex; line: 21  ungetindex(x::AbstractArray, dy, i...)  = (dx=zeros_like(x);setindex!(dx,dy,i...);dx)
          2264 ...oGrad/src/core.jl; fill_internal; line: 548  fill_internal{T}(x::AbstractArray{T},v,d::ObjectIdDict)=
           2244 ...oGrad/src/core.jl; fill_check; line: 551  fill_check(x,v,d::ObjectIdDict)=(haskey(d,x) ? d[x] : d[x]=fill_internal(x,v,d))
            2232 ...oGrad/src/core.jl; fill_internal; line: 548  fill_internal{T}(x::AbstractArray{T},v,d::ObjectIdDict)=
             2178 array.jl; fill!; line: 193

      348  ...rc/linalg/matmul.jl; anonymous; line: 2

      1019 ...utoGrad/src/util.jl; anonymous; line: 22  gexp = :(dy->dy.*$(_d[i]))
       203 arraymath.jl; .*; line: 125
       157 no file; .*; line: 0
        113 ...utoGrad/src/core.jl; r; line: 127  result = f(argvals...; kwargs...)
         109 ...se/sparsematrix.jl; .*; line: 1026
       150 no file; .==; line: 0
        139 ...utoGrad/src/core.jl; u; line: 408  u(x...; o...)=f(map(getval,x)...; o...)
         126 broadcast.jl; .==; line: 363
       473 sparse/sparsematrix.jl; .*; line: 1026
        430 broadcast.jl; broadcast!; line: 246

      232  ...utoGrad/src/util.jl; new_fun; line: 189  result = gradfun(dy)

      222  ...utoGrad/src/util.jl; new_fun; line: 196  return sum(result, d)

	- Hypotheses:
	- closures slow.
	- untyped functions slow.
	- memory allocation slow.
	- two types of memory alloc: user code, grad code.
	- for user code, let the user do it, support overwriting ops (careful about non-Node arrays)
	- for grad code, keep around a tape after it is used?  how about args in closures?

	* TODO:
	# use TypeNode <: Type instead of Node{Type}?
	# profile mnist
	# work on efficiency (are closures efficient? nothing instead of zero arrays?).
	# work on overwriting: 3arg (overwriting) functions or julia v0.5 overwrite syntax or InplaceOps.
	# check out JuliaDiff/ReverseDiffSource: cannot handle while loops? http://www.juliadiff.org/
	# Julia 5 already supports function types making Fn{} unnecessary: julia> typeof(sin) => Base.#sin
	# Julia 5 in-place syntax does not work for matmul, or .+ (unless you write x .= (+).(x,a)) yet.
	# implement quick_grad_check for mnist.
	# rnn example?
	# support CudaArrays.
	# rnn example?
	# implement convenience_wrappers (jacobian etc)?
	# copying, reshaping, subarrays and concatenation?
	# finish all functions.
	# write documentation and publish.
	# optimize dy.*1 gradients, but be careful not to pass back same array multiple times if we are going to use accumulators.

2016-08-17  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# Julia v5.0 compatibility.

	* q: Node{Type} vs TypeNode <: Type

	We chose the first (a single parametric type) whereas the Python
	implementation uses the second (many types, I don't think python
	supports parametric types).  In the first, all types are children
	of Node, in the second we can hang them anywhere we want in the
	type hierarchy. Today Emre Yolcu suggested a possible advantage of
	the second approach: in cases where one Julia function calls a
	lower level Julia function in Base, we could get away with
	defining only the lower level function as primitive as long as we
	make AbstractFloatNode a subtype of AbstractFloat, and the Julia
	functions are written for a supertype of AbstractFloat, for
	example.  It is not possible to make Node{Float32} a subtype of
	anything other than Node and Any.  This would make it easier to
	cover groups of functions such as (*)->A_mul_B!->gemm_wrapper! or
	vcat->cat by only letting us define the lowest level one.  We'd
	need to cover arrays, matrices, vectors, tuples, dicts, floats.
	It would also solve the problem of not being able to define
	Node{A<:AbstractArray{T<:Number}}.  The current Node type used by
	core.jl would have to be a big union instead of a supertype, which
	may effect efficiency.  On the downside we may have to define many
	types and the "lowest level" we depend on may change in the next
	Julia version.

2016-08-16  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# fix namespace for runtests.jl.
	# solve testing problem with zerograd args (sum, airy)

2016-08-15  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# extend defgrads so it can handle manual definitions as well.
	# implement reductions: sum, vecnorm
	# implement arraymath functions (transpose etc)
	# test mnist etc. more examples.
	# implement zero-one loss.
	# write mnist loader.
	# implement zerograd for one of the arguments.

2016-08-14  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# reorganize gradients mirroring base.
	# handle broadcast.jl.

2016-08-13  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# use Grad or some other name instead of Val.
	# split functions based on what type of args they accept.
	# implement broadcasting functions (finish unbroadcast).
	# implement matrix multiplication.

2016-08-12  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# write gradcheck.
	# implement unbroadcast
	# implement/test 2arg functions.

2016-08-11  Deniz Yuret  <dyuret@ku.edu.tr>

	* DONE:
	# @primitive should be type specific.
	# sum_outgrads should not overwrite its arguments: e.g. + may have passed the same dy back to multiple places.
	# separate tests, examples, and gradients.
	# (w,b)=params does not work, implement iterators?

